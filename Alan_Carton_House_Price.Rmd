---
title: "Advanced R - Individual Assignment"
author: "Alan Carton"
date: "23 May 2019"
output:
  pdf_document: default
  html_document: null
  toc: yes
  toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Variables that may need to be set when running
## Setting the working directory.
setwd("C:/Users/ajukg/R_Advanced/Assignments/Alan_Carton_Project/House_Price/House_Price")
# Location of data directory. Can be relative to Woking directory or absolute.
csv_file_dir <- "../Data/"
str_train_file <- paste(csv_file_dir, "house_price_train.csv", sep="")
str_test_file <- paste(csv_file_dir, "house_price_test.csv",sep="")
str_output_file <- paste(csv_file_dir, "AlanCarton_HousePrice_Predictions.csv",sep="")

# Setting type of Error validation, Type of Lots to use and initial values
err_type_ <- "MAPE"         # Options "MAPE" or "RMSE"
use_log <- 1                # Options -1 (No Log), 0 (log) or 1(log1p)

# Just setting initial values 
err_value_ <- 0
model_run <- "Baseline-1"


# Loading libraries that are required through program
library(ggplot2)
library(plyr)
library(dplyr)     # To compute the `union` of the levels.
library(png)       # To include images in this document.
library(knitr)     # To include images inline in this doc.
library(moments)   # Skewness
library(e1071)     # Alternative for Skewness
library(glmnet)    # Lasso
library(caret)     # To enable Lasso training with CV.
library(data.table) # for logging results
library(xgboost)
library(rpart)
library(ridge)
library(FSelector)
library(spFSR)
library(mlr)

seed_val <- 3012
set.seed(seed_val)

# R
starttime <- Sys.time()
head <- data.table(err_id=numeric(), model=character(), alpha=numeric(), rmse=numeric(), mae=numeric(), mape=numeric());

colResults <- head;
lassoResults <- data.table(err_id=numeric(), model=character(), alpha=numeric(), lambda=numeric(), train_error=numeric());

# Setting base model.
lm_model<-0

```

# Introduction

Objective: Predict House Prices
 Data: house_price_train.csv & house_price_test.csv
 Due date: May 23rd
Submission:
  Printable Report (pdf/word/ppt)
  R codes, notebooks, all in a unique structure folder with relative paths
  Test Prediction (csv)



## R Functions

Functions used commonly through the project are stored here.

```{r project_functions, message=FALSE, warning=FALSE}
# Initially had functions stored as external file for clarity but as was for advanced coding class project, including here instead.
# source('Functions.r')

# Functions to print Time and Elapsed Time
print_elapsed <- function(start_){
  print(paste("Elapsed :: ",get_elapsed(start_)))
}

get_elapsed <- function(start_){
  return(Fmt(Sys.time()- start_))
}

Fmt <- function(x) UseMethod("Fmt")
Fmt.difftime <- function(x) {
     units(x) <- "secs"
     x <- unclass(x)
     NextMethod()
}
Fmt.default <- function(x) {
   y <- abs(x)
   sprintf("%s%02d:%02d:%02d:%02d", 
      ifelse(x < 0, "-", ""), # sign
      y %/% 86400,  # days
      y %% 86400 %/% 3600,  # hours 
      y %% 3600 %/% 60,  # minutes
      y %% 60 %/% 1) # seconds
}

print_time <- function(sec){
  # Function to print Sys.time() variables in "yyyy-mm-dd h:m:s" format
  print (format(sec, "%X"))
}

# Function to split a dataset into training and validation.
splitdf <- function(dataframe, seed=42) {
  # Function originally had 1/1.5 split for training.  I prefer 75/25 or 80/20 so changed to .8
  
  index <- 1:nrow(dataframe)
  trainindex <- sample(index, trunc(length(index)*.75))
  trainset <- dataframe[trainindex, ]
  testset <- dataframe[-trainindex, ]
  list(trainset=trainset,testset=testset)
}

# Get Error Values
get_error_value <- function(real, predicted){
  err_value_ <- 0
  if(err_type_=="RMSE"){
    err_value_ <- rmse(real, predicted);
  }
  else if(err_type_=="MAPE"){
    err_value_ <- mape(real, predicted);
  }
  else{
    err_value_ <- mae(real, predicted);
  }
  
  return(err_value_)
  
}

# Additional Functions
reset_data <- function(){
  
  training <<- tmp_training;
  test <<- tmp_test;
  validation <<- tmp_validation;
  
  colResults
  
}

get_best_model <- function(err_type){
  
  val_ <- 0
  err_type = tolower(err_type)
  if(err_type=="mape"){
    val_ <- colResults[colResults$mape==min(colResults$mape)]
  }
  if(err_type=="mae"){
    val_ <- colResults[colResults$mae==min(colResults$mae)]
  }
  if(err_type=="rmse"){
    val_ <- colResults[colResults$rmse==min(colResults$rmse)]
  }
  # Return sorted by descending id.
  return(val_[order(-err_id),])#[1])
  
}

set_backup <- function(model_){
  
  model_run <<- model_;
  tmp_test <<- test;
  tmp_validation <<- validation;
  tmp_training <<- training;
}

check_best_model <- function(err_type){
  best_ <- get_best_model(err_type)
  blnRtn <- T;
  if (best_$model[1]!=model_run){
    print("Model error deteriorated, reseting to last set of columns.")
    reset_data();
    blnRtn <- F;
  }
  return(blnRtn)
}

# Log of Values, depending on scale.
exp_value <- function(value, scale){
  rtn_ <- 0
  if (scale==0){
    rtn_ <- exp(value);
  }
  else if (scale==1){
    rtn_ <- exp(value)-1;
  }
  else{
    rtn_ <- value;
  }
  return(rtn_)
}

log_value <- function(value, scale){
  rtn_ <- 0
  if (scale==0){
    rtn_ <- log(value);
  }
  else if (scale==1){
    rtn_ <- log1p(value);
  }
  else{
    rtn_ <- value;
  }
  return(rtn_)
}

# Functions to generate Error Statistics
# Functions will generate Exponential Values where real values use log.
mae<-function(real, predicted){
  tmp_real_ <- exp_value(real, use_log)
  tmp_predicted_ <- exp_value(predicted, use_log)
  return(mean(abs(tmp_real_-tmp_predicted_)))
}

mape<-function(real,predicted){
  tmp_real_ <- exp_value(real, use_log)
  tmp_predicted_ <- exp_value(predicted, use_log)
  return(mean(abs((tmp_real_-tmp_predicted_)/tmp_real_)))
}

rmse<-function(real,predicted){
  tmp_real_ <- exp_value(real, use_log)
  tmp_predicted_ <- exp_value(predicted, use_log)
  return(sqrt(mean((tmp_real_-tmp_predicted_)^2)))
}

# Returns the glm model so can be used in other situations.  I wanted to try a prediction using the model..
get_glm_model <- function(training_dataset, validation_dataset, title, method="glm"){
  train_control_config <- trainControl(method = "repeatedcv", 
                                       number = 5, 
                                       repeats = 1, #allowParallel =T,
                                       returnResamp = "all")
  
  this.model <- FALSE
  
  if (method=="glm"){
    # Fit a glm model to the input training data
    this.model <-caret::train(price ~ ., 
                              data = training_dataset, 
                              method = method, 
                              metric = err_type_,
                              preProc = c("center", "scale"),
                              na.action = na.omit,
                              trControl=train_control_config) 
  }
  else if (method=="rf"){
    # Fit a Random Forest model to the input training data
    train_control_config <- trainControl(method = "none", allowParallel =T)
    
    this.model <- caret::train(price ~ ., data = training,
                               method = "rf",
                               ntree = 1000,
                               metric = err_type_,
                               trControl = train_control_config,
                               # num.threads = (availableCores()-1),
                               tuneGrid = data.frame(mtry = 6))
    
  }
  else if (method=="rpart"){
    # Fit a Random Forest model to the input training data
    
    train_control_config <- trainControl(method = "repeatedcv", 
                                         number = 5, 
                                         repeats = 1,
                                         returnResamp = "all")
    
    this.model <- caret::train(price ~ ., data = training,
                               method = "rf",
                               ntree = 1000,
                               metric = err_type_,
                               trControl = train_control_config,
                               tuneGrid = data.frame(mtry = 6))
    
  }
  else if (method=="xgboost"){
    
    #formulaString <- as.formula(paste('price ~ ', paste(names(training), collapse="+")))
    
    train_control_config <- trainControl(method="repeatedcv", number=3, repeats=1)
    
    gb.tuneGrid <- expand.grid(
      eta = c(0.3,0.4,0.5), 
      nrounds = c(50,100,150), 
      max_depth = 2:4, gamma = 0, 
      colsample_bytree = 0.8, 
      min_child_weight = 1, 
      subsample=1)
    
    this.model <- caret::train(price ~ .,
                               data=training_dataset, 
                               method="xgbTree",
                               metric = err_type_,
                               trControl=train_control_config, 
                               #nthread=(availableCores()-1),
                               tuneGrid=gb.tuneGrid
    )
  }
  else{
    # Fit a glm model to the input training data
    this.model <-caret::train(price ~ ., 
                              data = training_dataset, 
                              method = method, 
                              metric = err_type_,
                              preProc = c("center", "scale"),
                              na.action = na.omit,
                              #num.threads = (availableCores()-1),
                              trControl=train_control_config) 
  }
  
  
  
  
  return(this.model)
}

  # Functions to generate models.
lm.model <- function(training_dataset, validation_dataset, title_) {
  # Create a training control configuration that applies a 5-fold cross validation
  
  this.model <- get_glm_model(training_dataset, validation_dataset, title_)
  
  # Prediction
  
  pred_ <- predict(this.model, validation_dataset)
  pred_[ is.na(pred_)] <- 0 # To avoid null predictions
  real_ <- validation_dataset$price
  
  err_value_ <- get_error_value(real_, pred_)
  # Error of the model
  rmse_ <- rmse(real_, pred_)
  mape_ <- mape(real_, pred_)
  
  # Error in terms of the mean deviation between the predicted value and the price of the houses
  mae_ <- mae(real_, pred_)
  
  lm_model<<-this.model
  
  colResults <<- rbind(colResults, data.table(err_id = nrow(colResults)+1, model = title_, alpha = 0, rmse = rmse_, mae=mae_, mape=mape_));
  
  plot_values(pred_, real_, title_, "lm", err_value_, mae_)
  
}

run_model <- function(title, model_){
  
  model <- get_glm_model(training, validation, title, model_)
  model.model <- model
  
  pred_ <- predict(model, validation)
  real_ <- validation$price
  # ERROR of the model
  
  # Compute Metrics
  rmse_ <- rmse(real_, pred_)
  mape_ <- mape(real_, pred_)
  
  # Error in terms of the mean deviation between the predicted value and the price of the houses
  mae_ <- mae(real_, pred_)
  
  # Alpha is not important for this type of models.
  colResults <<- rbind(colResults, data.table(err_id = nrow(colResults)+1, model = model_, alpha = 0, rmse = rmse_, mae=mae_, mape=mape_));
  
  model.rmse = rmse_
  model.price_error <- mae_
  model.mape <- mape_
  err_value_ <- 0
  
  if(err_type_=="RMSE"){
    err_value_ <- rmse_;
  }
  if(err_type_=="MAPE"){
    err_value_ <- mape_;
  }  
  
  # Plot the predicted values against the actual prices of the houses
  plot_values(pred_, real_, model_, "glm", err_value_, mae_)
  
  return(list(model=model.model, pred=pred_))
  
}

plot_values <- function(predicted_, observed_, model_, method_, err_value_, mae_){
  # Plot the predicted values against the actual prices of the houses
  my_data <- as.data.frame(cbind(predicted=exp_value(predicted_,use_log), observed=exp_value(observed_,use_log)))
  
  ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = method_) +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste(model_, ' : ', err_type_, ' : ', format(round(err_value_, 4), nsmall=4), ' --> Price ERROR:', format(round(mae_, 0), nsmall=0), 
                          ' :: ', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
  
}

# Boxplots but the scale was very bad for some.
ShowBoxplot <- function(df, col_names) {
  boxplot(df[,col_names], xaxt="n", coef = 3, outlier.shape=NA, outline=FALSE)
  
  # x axis with ticks but without labels
  # Plot x labs at default x position
  text(x =  seq_along(col_names), y = par("usr")[3] - (par("usr")[4] - par("usr")[3])/30, srt = 45, adj = 1,
       labels = colnames(dataset[col_names]), xpd = TRUE)
}

# Data_CleanupFunctions
# Functions for Listing columns for nulls from any given dataset
list_na_cols <- function(df){
  na.cols <- which(colSums(is.na(df)) > 0)
  print(paste('There are', length(na.cols), 'columns with missing values'))
  if(length(na.cols)>0){
    print(sort(colSums(sapply(df[na.cols], is.na)), decreasing = TRUE))
  }
  
  return(na.cols)
}

# Set Factor Values for Columns
clean_set_factor <- function(df, col_name, level_vals){
  
  cols <- colnames(df)==col_name
  df[,cols] <- factor(df[,cols], levels=level_vals)
  
  return(df)
}

#function to count the number of nulls in a given dataset column
are_nulls <- function(x_name){
  return(sum(is.na(x_name)) > 0)
}

# Function to retrieve list of numeric columns from a dataset, with option to include / exclude the target_column for models.
get_numeric_cols <- function(df_, target_column){
  ret_val <- colnames(df_[, sapply(df_, is.numeric) & colnames(df_) != target_column])
  return(df_[ret_val])
}

# Generalising the above in a function to only return a type or all except a type.
get_columns_of_type <- function(data_, col_type){
  col_types <- sapply(names(data_), function(x){class(data_[[x]])});
  ret_names <- names(col_types[col_types == col_type])
  return(ret_names)
}

get_columns_not_of_type <- function(data_, col_type){
  col_types <- sapply(names(data_), function(x){class(data_[[x]])});
  ret_names <- names(col_types[col_types != col_type])
  return(ret_names)
}


```

# Data Reading and preparation
The dataset is offered in two separated files, one for the training and another one for the test set.  They need to be merged for cleaning and data treatment, and split at a later date back to the original files. 

```{r data_reading, message=FALSE, warning=FALSE}

# First read the individual datasets into the code
original_training_data = read.csv(file = file.path(str_train_file))
original_test_data = read.csv(file = file.path(str_test_file))

# Datasets merged for cleaning purposes, will be split again post cleaning for train / validation / test.
original_test_data["price"] <- 0
dataset <- rbind(original_training_data, original_test_data)

na.cols <- list_na_cols(dataset)
```

To avoid applying the Feature Engineering process two times (once for training and once for test), you can just join both datasets (using the `rbind` function), apply your FE and then split the datasets again. However, if we try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `price`. Therefore, we first create this column in the test set and then we join the data

Let's now visualize the dataset to see where to begin

```{r dataset_visualization, message=FALSE, warning=FALSE}

#
# id - identifier. Qty Records 217277
# Consisted of 21 Columns before cleanup and feature generation.

# For the purposes of visualization, Summary and head were used to identify the data and patterns.  No other visualization was used other than boxplots for outliers, which appear later.

# summary(dataset) # Commented out as takes lot of screenspace.
head(dataset)


```

We can see some problems just by taking a look to the summary: the dataset has no missing values, there are some categorical columns codified as numeric, it has different scales for the feature values. In addition, I will take a  look to the data to detect more subtle issues: correlation between features, skewness in the feature values...

# Data Cleaning

The definition of "meaningless" depends on data and intuition. A feature can lack any importance because you know for sure that it does not going to have any impact in the final prediction (e.g., the id of the house). In addition, there are features that could be relevant but present wrong, empty or incomplete values (this is typical when there has been a problem in the data gathering process). For example, the feature `Utilities` present a unique value, consequently it is not going to offer any advantage for prediction.

We remove meaningless features and incomplete cases.

```{r eliminate_unique_data, message=FALSE, warning=FALSE}

# Date and id are deemed to be meaningless, so will be removed as add nothing to data.

dataset <- dataset[,-which(names(dataset) == "id")]
dataset <- dataset[,-which(names(dataset) == "date")]

# Some additional columns will be removed later as may be used for interpretation before removing.

```

## Hunting NAs

Our dataset has no missing values, so we do not have to worry about imputing details.

```{r , message=FALSE, warning=FALSE}

# 0 Columns have NA Values.
list_na_cols(dataset)

```

## NAs

The data did not show any Null / NA Values

Now we look at Categorical and Numerical values to make sure that they are correctly identified for modelling.  Numerical values are only good for size, price, etc, but for things like Bedrooms, Bathrooms, Quality Scales (1-10) ... they are better represented as categorical values, so will be set.


```{r numbers_as_Categorical, message=FALSE, warning=FALSE}
# Some numerical values need to be treated as categorical, and need to be replaced with factor levels.

filterCols <- c("bedrooms", "bathrooms", "floors", "waterfront", "view","condition", "grade", "yr_built", "yr_renovated", "zipcode")

# Some numeric fields should be factors (i.e. Number of Rooms, etc.)
# These do not have factors, but should be set to the existing values.
for(col_name in filterCols){
  level_vals <- sort(unique(dataset[col_name])[,col_name])
  dataset[,col_name] <- factor(dataset[,col_name], levels=level_vals)
  
}
# Double checking that no null values after categorization of data.

na.cols <- list_na_cols(dataset)

```

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: Rooms, Bathrooms etc. What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r factorize_features, message=FALSE, warning=FALSE}


# Calculating some new features here as required earlier in the process.
dataset["TotalFlArea"] <- dataset$sqft_above + dataset$sqft_basement

dataset["sqft_building15"]<- dataset$sqft_living15 + dataset$sqft_lot15
dataset["sqft_building"]<-dataset$sqft_living + dataset$sqft_lot

dataset["has_basement"]<-dataset$sqft_basement>0
dataset["increased15"]<-(dataset$sqft_building15 - dataset$sqft_building)>0
dataset["unchanged15"]<-(dataset$sqft_building15 - dataset$sqft_building)==0

filterCols <- c("increased15", "unchanged15", "has_basement")

# Some numeric fields should be factors (i.e. Number of Rooms, etc.)
# These do not have factors, but should be set to the existing values.
for(col_name in filterCols){
  level_vals <- sort(unique(dataset[col_name])[,col_name])
  dataset[,col_name] <- factor(dataset[,col_name], levels=level_vals)
  
}

```

## Outliers
We will now focus on numerical values. If `NAs` where the natural enemy of categorical values, the main problem with numerical values are outliers (values which largely differ from the rest). Outliers can mislead the training of our models resulting in less accurate models and ultimately worse results.

In this section I will identify outliers to then properly deal with them. If we summarize the dataset, we can see variables which "Max." is much larger than the rest of values. These features are susceptible of containing outliers. Nevetheless, the easiest way to detect outliers is visualizing the numerical values; for instance, by `boxploting` the column values.

Using, the `boxplot` function can eliminate the outliers. However, if you apply it with the default values it is going to eliminate too much of them. You can adapt its working with the `outlier.size` param which is recommended to be set to at least 3. Another thing you can do is to compare the columns with outliers to the target variable (`price`) to visually check if there are some extreme values and just consider those as outliers.

## Dealing with Outliers
Although they could possibly have been ignored, I decided to set all outliers to their respective minimum outlier value.  They would still be showing as outlier, but within an approved magnitude.  Setting to 0 or a Mean / Median did not appear to me to be a suitable solution in this instance.  By setting to the minimum value of the outlier range, it both classes them as outliers, but minimising their effect on the dataset.

```{r outlier_detection, message=FALSE, warning=FALSE}

library(outliers)

# Function to retrieve list of numeric columns from a dataset, with option to include / exclude the target_column for models.
numeric_cols <- colnames(get_numeric_cols(dataset, 'price'))

# Using the boxplot stats function to retrieve the list of outlier values.
# Not possible to remove outliers, as would effect the dataset.
# By setting the values to the minimum outlier value accepted, it is still identifying them as outliers but minimising their effect on the dataset.

for(col_name in numeric_cols){
  coef_val <- boxplot.stats(dataset[,col_name], coef = 3)$out
  min_outlier <- min(coef_val, na.rm=FALSE)
  # locate the outlier records, and set their value to the min
  dataset[dataset[,col_name]  %in% coef_val , col_name] <- min_outlier

}

```

## Now Show Boxplots for outlier columns.  Will still show as outliers, but within approved distance.

Boxplots are shown in one graphic for all numeric columns.

```{r outliers_boxplots, message=FALSE, warning=FALSE}

# Basic box plot
ShowBoxplot(dataset, numeric_cols)

```


## Skewness

We now need to detect skewness in the Target value : Price. Let's see what is the effect of skewness on a variable, and plot it using ggplot. The way of getting rid of the skewness is to use the `log` (or the `log1p`) of the values of that feature, to flatten it. To reduce right skewness, take roots or logarithms or reciprocals (x to 1/x). This is the commonest problem in practice. To reduce left skewness, take squares or cubes or higher powers.

```{r check_skewness, message=FALSE, warning=FALSE}

df <- rbind(data.frame(version="price",x=original_training_data$price),
            data.frame(version="log1p(price)",x=log_value(original_training_data$price,1)),
            data.frame(version="log(price)",x=log_value(original_training_data$price,0))
            )

ggplot(data=df) +
  facet_wrap(~version,ncol=3,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)


```

We can see that applying log or log1p, brings the price to normal distribution, so we will transform the variable using log for fit and then exp for predictions.

The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform the same log transformation

The same was achieved with Log and Log1p so using Log1p for fit and predictions, and appropriate exp for error checking using use_log<-1 setting.  This was required as some of the numerical variables contained 0, and when log was applied the number changed to NA which was not appropriate.  As Log1p gave same result, it was deemed more appropriate, rather than replacing NA with 0 which was alternate.

Setting threshold for the skewness in 0.75. 


```{r set_skew_threshold, message=FALSE, warning=FALSE}

# Log transform the target for official scoring
# As problems with MAPE decided to not log price

dataset$price <- log_value(dataset$price, use_log)

skewness_threshold = 0.75

```

Now, let's compute the skewness of each feature that is not 'factor' nor 'character'. So, I'm only interested in continuous values. One possible way of doing it is the following: First, lets determine what is the 'class' or data type of each of my features.

What we want to determine is the class of each column or feature, and to do so, we use the `class` method from R. We will pass the actual column or feature from our dataset (dataframe).  With that information, we need to calculate the skewness of each column whose name is our list of __factor__ (or categorical) features. We use the `sapply` method again, to compute the skewness of each column whose name is in the list of `numeric_columns`.

```{r apply_skewness_nonfactors, message=FALSE, warning=FALSE}
column_types <- sapply(names(dataset), function(x) {
  class(dataset[[x]])
}
)

# Getting the list of columns that are not factors
numeric_columns <- names(column_types[column_types != "factor"])

numeric_cols = numeric_cols[numeric_cols!="lat"]
numeric_cols = numeric_cols[numeric_cols!="long"]

skew <- sapply(numeric_cols, function(x) {e1071::skewness(dataset[[x]], na.rm = T)})

# transform all variables above a threshold skewness, using the same function and methods as other datapoints - either log or log1p. log1p works better as value for some causes nulls. list_na_cols(dataset)

skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew)) {
  dataset[[x]] <- log_value(dataset[[x]], use_log)
}

```

What we do need to make now is to apply the log to those whose skewness value is below a given threshold that we've set in 0.75. We should test different hypothesis with our threshold too, using the log_value function with the use_log variable.

```{r apply_new_Skew_threshold, message=FALSE, warning=FALSE}

# transform all variables above a threshold skewness, using the same function and methods as other datapoints - either log or log1p.
skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew)) {
  dataset[[x]] <- log_value(dataset[[x]], use_log)
}

```

# Train, Validation Spliting
To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.

We are going to split the modified dataset into Test, train and validation for the later evaluation of our regression models

```{r train_test_val_split, message=FALSE, warning=FALSE}

len_training = NROW(original_training_data)
len_test = NROW(original_test_data)
len_all = NROW(dataset)

training_data <- head(dataset, n=len_training)
test <- tail(dataset, n=len_test)

splits <- splitdf(training_data, seed=1)
training <- splits$trainset
validation <- splits$testset

rm(training_data)
rm(splits)

tmp_test <- test
tmp_validation <- validation
tmp_training <- training

```

# Feature Selection: Baseline model
I have cleaned the data, and categorized numerical variables, and tidied up outliers. I have generated some extra features, but now need to check which of those features can be removed as they do not add to the error value to be used for basis of modelling.  Initially running with all columns for baseline.

```{r baseline_1, message=FALSE, warning=FALSE}

set_backup("Baseline-1")
# Using all columns

glmmodel <- lm.model(training, validation, model_run)
glmmodel

# This checks to see if this latest model produced lower error score.  If not will reset the columns to the previous best set of columns.
r <- check_best_model(err_type_)

```
### Current Error table results.

```{r baseline_1_results, message=FALSE, warning=FALSE}
print(colResults)

```

# Feature Selection
We here start the Feature Selection.

## Filtering Methods
We will rank the features according to their predictive power according to the methodologies seen in class: the Chi Squared Independence test, Spearman and the Information Gain.

### Chi-squared Selection
let's use the chisq.test included in the base package of R, to measure the relationship between the categorical features and the output. Only those.

```{r baseline_1_chisquare, message=FALSE, warning=FALSE}
# Compute the ChiSquared Statistic over the factor features ONLY
features <- colnames(training[, sapply(training, is.factor) & colnames(training) != 'price'])

chisquared <- data.frame(features, statistic = sapply(features, function(x) {
  chisq.test(training$price, training[[x]])$statistic
}))

# Plot the result, and remove those below the 1st IQR (inter-quartile-range) --aggressive
par(mfrow=c(1,2))
boxplot(chisquared$statistic)
bp.stats <- as.integer(boxplot.stats(chisquared$statistic)$stats)   # Get the statistics from the boxplot

chisquared.threshold = bp.stats[2]  # This element represent the 1st quartile.
text(y = bp.stats, labels = bp.stats, x = 1.3, cex=0.7)
barplot(sort(chisquared$statistic), names.arg = chisquared$features, cex.names = 0.6, las=2, horiz = T)
abline(v=chisquared.threshold, col='red')  # Draw a red line over the 1st IQR

```



Now, we can test if this a good move, by removing any feature with a Chi Squared test statistic against the output below the 1 IQR.  Then we re-run the baseline model to see if the resulting error has improved from the previous baseline.  If it has removed, then the columns are dropped for future modelling.  If it has worsened, the columns are restored for future modelling so that we continue to work on best set of columns.

```{r baseline_2, message=FALSE, warning=FALSE}

# Determine what features to remove from the training set.
features_to_remove <- as.character(chisquared[chisquared$statistic < chisquared.threshold, "features"])
col_names_remaining <- !colnames(training) %in% features_to_remove

print("Columns marked for potential removal:")
print(features_to_remove)
```


### Rerun model to check validity of feature removal.
We can now re-run the model to remove the items identified in the factor / chi-square removal to see if they improve the baseline error.

```{r baseline_2_model, message=FALSE, warning=FALSE}

set_backup("Baseline-2")

training <- training[col_names_remaining]
validation <- validation[col_names_remaining]
test <- test[col_names_remaining]

glmmodel <- lm.model(training, validation, model_run)
glmmodel

# This checks to see if this latest model produced lower error score.  If not will reset the columns to the previous best set of columns.
r <- check_best_model(err_type_)

```

### Current Error table results.

```{r baseline_2_results, message=FALSE, warning=FALSE}
# Checking the Error values for comparison about whether or not to retain or remove the column suggestions.
print(colResults)
```
### MAPE error improved
As the MAPE improved, we can drop the columns.

### Now, Try feature selection of numerical variables with Spearman's correlation.
For Numerical variables, we can  measure its relation with the outcome through the Spearman's correlation coefficient, and remove those with a lower value. Let's repeat the same process we did with the Chi Square but modifying our code to solely select numerical features and measuring Spearman'.

```{r baseline_3_chisquare, message=FALSE, warning=FALSE}
# Compute the ChiSquared Statistic over the factor features ONLY
features <- colnames(training[, sapply(training, is.factor) & colnames(training) != 'price'])

chisquared <- data.frame(features, statistic = sapply(features, function(x) {
  chisq.test(training$price, training[[x]])$statistic
}))

# Plot the result, and remove those below the 1st IQR (inter-quartile-range) --aggressive
par(mfrow=c(1,2))
boxplot(chisquared$statistic)
bp.stats <- as.integer(boxplot.stats(chisquared$statistic)$stats)   # Get the statistics from the boxplot

chisquared.threshold = bp.stats[2]  # This element represent the 1st quartile.
text(y = bp.stats, labels = bp.stats, x = 1.3, cex=0.7)
barplot(sort(chisquared$statistic), names.arg = chisquared$features, cex.names = 0.6, las=2, horiz = T)
abline(v=chisquared.threshold, col='red')  # Draw a red line over the 1st IQR

```

Let's select the features for the model, using the detail from Chi Sq. section above for feature columns.

```{r baseline_3_remove, message=FALSE, warning=FALSE}

# Determine what features to remove from the training set.
features_to_remove <- as.character(chisquared[chisquared$statistic < chisquared.threshold, "features"])
col_names_remaining <- !colnames(training) %in% features_to_remove

print("Removing : ")
print(features_to_remove)
```

### Rerun model to check validity of feature removal.
We can now re-run the model to remove the items identified in the factor / chi-square removal to see if they improve the baseline error.

```{r baseline_3_model, message=FALSE, warning=FALSE}

set_backup("Baseline-3")

training <- training[col_names_remaining]
validation <- validation[col_names_remaining]
test <- test[col_names_remaining]

glmmodel <- lm.model(training, validation, model_run)
glmmodel

# This checks to see if this latest model produced lower error score.  If not will reset the columns to the previous best set of columns.
r <- check_best_model(err_type_)

```

### Current Error table results.

```{r baseline_3_results, message=FALSE, warning=FALSE}
# Checking the Error values for comparison about whether or not to retain or remove the column suggestions.
print(colResults)
```

### MAPE did not improve
As the MAPE error did not lower from removal of the columns, we reset the column selection to the previos set of columns for the better results.

### ChiSquared for numeric values
Now we check the numerical values to identify which columns could be discarded or not.  The target variable needs to be removed from the removal selection, and the method used for correlation is Spearman.

```{r baseline_4_spearman, message=FALSE, warning=FALSE}
# Compute the ChiSquared Statistic over the factor features ONLY
features <- colnames(training[, sapply(training, is.numeric) & colnames(training) != 'price'])

spearman <- data.frame(features, statistic = sapply(features, function(x) {
cor(training$price, training[[x]], method='spearman')
}))

# Plot the result, and remove those below the 1st IQR (inter-quartile-range) --aggressive
par(mfrow=c(1,2))
boxplot(abs(spearman$statistic))
bp.stats <- boxplot.stats(abs(spearman$statistic))$stats   # Get the statistics from the boxplot
text(y = bp.stats, 
labels = sapply(bp.stats, function(x){format(round(x, 3), nsmall=3)}), # This is to reduce the nr of decimals
x = 1.3, cex=0.7)

spearman.threshold = bp.stats[2]  # This element represent the 1st quartile.

barplot(sort(abs(spearman$statistic)), names.arg = spearman$features, cex.names = 0.6, las=2, horiz = T)
abline(v=spearman.threshold, col='red')  # Draw a red line over the 1st IQR
```

### Columns identified for removal.
```{r baseline_4_columns, message=FALSE, warning=FALSE}

print("Columns marked for potential removal:")
print(features_to_remove)
print("Columns marked to use:")
print(names(training[col_names_remaining]))

```

### Rerun model to check validity of feature removal.
We can now re-run the model to remove the items identified in the factor / chi-square removal to see if they improve the baseline error.

```{r baseline_4_model, message=FALSE, warning=FALSE}

set_backup("Baseline-4")

training <- training[col_names_remaining]
validation <- validation[col_names_remaining]
test <- test[col_names_remaining]

glmmodel <- lm.model(training, validation, model_run)
glmmodel

# This checks to see if this latest model produced lower error score.  If not will reset the columns to the previous best set of columns.
r <- check_best_model(err_type_)

```

### Current Error table results.

```{r baseline_4_results, message=FALSE, warning=FALSE}
# Checking the Error values for comparison about whether or not to retain or remove the column suggestions.
print(colResults)
```


### Information Gain Selection

This part is equivalent to the Chi Squared, but with another metric. So, the coding is very much equivalent.

```{r baseline_5_information.gain, message=FALSE, warning=FALSE}

left <- training$price

# This computes the information gain and stores in Dataset
weights <- data.frame(information.gain(left ~ .,training))
weights["col_names"] <- rownames(weights)

# Recommended features to keep.
features_to_use <- weights$col_names[weights$attr_importance >= 0.05]

```

### Columns identified to be retained after Information Gain checks.

```{r baseline_5_columns, message=FALSE, warning=FALSE}

print("Columns marked to use:")
print(features_to_use)

```

### Rerun model to check validity of feature removal.
We can now re-run the model to remove the items identified in the factor / chi-square removal to see if they improve the baseline error.

```{r baseline_5_check_checkresults, message=FALSE, warning=FALSE}

set_backup("Baseline-5")

training <- training[features_to_use]
validation <- validation[features_to_use]
test <- test[features_to_use]

glmmodel <- lm.model(training, validation, model_run)
glmmodel

# This checks to see if this latest model produced lower error score.  If not will reset the columns to the previous best set of columns.
r <- check_best_model(err_type_)

```

### Current Error table results.

```{r baseline_5_results, message=FALSE, warning=FALSE}
# Checking the error values for comparison about whether or not to retain or remove the column suggestions.
print(colResults)
```

### Further feature selection checks

```{r final.Baseline.models, message=FALSE, warning=FALSE}

bin_price <- function(x, col_, bins){
  var_retd <- min(which(x[col_] <= bins))
  return(var_retd)
}

#> Loading required package: ParamHelpers
target_column <- "price"
retain_columns <- colnames(training)
knnWrapper    <- makeLearner("classif.knn", k = 5)

# Just get the numeric columns from the dataset to work out which better predictors.
tmpset <- get_numeric_cols(training, '')
col_bins <- c(1:max(tmpset[target_column] + 1))

tmpset["new_factor"] <- apply(tmpset, 1, function(x) min(which(x[target_column] <= col_bins)))

tmpset <- clean_set_factor(tmpset, target_column, c(unique(tmpset[target_column])))
tmpset <- tmpset[colnames(tmpset) != target_column]

classifTask   <- makeClassifTask(data = tmpset, target = "new_factor")
perf.measure  <- acc


spsaMod <- spFeatureSelection(
  task = classifTask,
  wrapper = knnWrapper,
  measure = perf.measure ,
  num.features.selected = 3,
  iters.max = 10,
  num.cores = 1)

```

### Final columns identified for potential removal.

```{r baseline_6_columns, message=FALSE, warning=FALSE}

cols_to_remove <- colnames(tmpset[!colnames(tmpset) %in% c(spsaMod$features, "new_factor")])
cols_to_retain <- colnames(training[!colnames(training) %in% cols_to_remove])

print("Columns marked for potential removal:")
print(cols_to_remove)
print("Columns marked to use:")
print(cols_to_retain)

```

### Rerun model to check validity of feature removal.
We can now re-run the model to remove the items identified in the factor / chi-square removal to see if they improve the baseline error.

```{r baseline_6_model, message=FALSE, warning=FALSE}

set_backup("Baseline-6")

training <- training[cols_to_retain]
validation <- validation[cols_to_retain]
test <- test[cols_to_retain]

glmmodel <- lm.model(training, validation, model_run)
glmmodel


```
### Final Model Selection if different to Baseline 6.

```{r baseline_final_results, message=FALSE, warning=FALSE}

# Depending on results, either keep or discard columns identified to remove.
result_ <- check_best_model(err_type_)

if (result_==F) {
  # If the latest model was not lowest error, rerun the model.
  model_run <<- "Baseline-Final";
  glmmodel <- lm.model(training, validation, model_run);
  glmmodel;
}

if (result_==T) {
  print("Final Linear Regression Model: Baseline-6");
}


```

### Current Error table results.

```{r baseline_6_results, message=FALSE, warning=FALSE}
# Checking the Error values for comparison about whether or not to retain or remove the column suggestions.
print(colResults)

```

### Final Columns to be used in all models.

```{r baseline_6_final_columns, message=FALSE, warning=FALSE}
print("Final columns used:")
names(test)

```

## Cleanup complete
We can see that some column removal resulted in a decrease in MAPE
In the end the result decreased for baseline linear regression models, and a final baseline of 0.1308502 was achieved.  

I will try to improve with Ridge / Lasso / Logistic Regression / Random Forest / Decision Tree and XG Boost.


### Ridge Regression

For this exercise, we are going to make use of the <a href="https://cran.r-project.org/web/packages/glmnet/index.html">`glmnet`</a> ridge library.

We will also use the same for Lasso, varying alpha values also.

```{r final.predictions.ridge, message=FALSE, warning=FALSE}

lambdas <- 10^seq(-3, 0, by = .05)
startprocess <- Sys.time()
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- caret::train(price ~ ., data = training, 
                          method = "glmnet", 
                          metric = err_type_,
                          trControl=train_control_config,
                          tuneGrid = expand.grid(alpha = 0, lambda = lambdas))

```

The parameter `alpha = 0` means that we want to use the Ridge Regression way of expressing the penalty in regularization. If you replace that by `alpha = 1` then you get Lasso.

We use a tuning grid to extract the best lambda values for Ridge.

#### Evaluation

Plotting the Error for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r ridge.plot, message=FALSE, warning=FALSE}

plot(ridge.mod)

```


### Plot for final Ridge Model.

```{r ridge_coefficients, message=FALSE, warning=FALSE}

plot(ridge.mod$finalModel)

```


### Now using the best lambda_ value, remodel.
Error value will be checked of this final model, and the values stored for later comparison.

```{r ridge.evaluation, message=FALSE, warning=FALSE}

  alpha_ <- 0
  lambda_ <- ridge.mod$bestTune[1,2]
  startprocess <- Sys.time()
  
  ridge.mod <- caret::train(price ~ ., data = training, 
                            method = "glmnet", 
                            metric = err_type_,
                            trControl=train_control_config,
                            tuneGrid = expand.grid(alpha = alpha_, lambda = lambda_))


  pred_ <- predict(ridge.mod, validation)
  pred_[is.na(pred_)] <- 0
  real_ <- validation$price

  err_value_ <- get_error_value(real_, pred_)
  # Error of the model
  rmse_ <- rmse(real_, pred_)
  mape_ <- mape(real_, pred_)
  
  # Error in terms of the mean deviation between the predicted value and the price of the houses
  mae_ <- mae(real_, pred_)

# Storing the values for final analysis
  colResults <<- rbind(colResults, data.table(err_id = nrow(colResults)+1, model = "ridge", alpha = alpha_, rmse = rmse_, mae=mae_, mape=mape_));
  
  # # Get errors - difference from Train to Val.
predictions_train <- pred_ # Previously taken from log.
train_ColToAnalyse  <-  real_
errors_train <- predictions_train - train_ColToAnalyse;

plot_values(pred_, real_, "Ridge", "glm", err_value_, mae_)

```

### Rank the variables according to the importance attributed by the model.

```{r plot_ridge, message=FALSE, warning=FALSE}

# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features

```

### Lasso Regresion

The only thing that changes between Lasso and Ridge is the `alpha` parameter. The remaining part of the exercise is equivalent.

### Now for Lasso.
For Lasso, we also use tune grid, but vary the alpha from 0.01 to 1, and also using the previous lambda grid.

```{r prepare_submission, message=FALSE, warning=FALSE}
# Now for Lasso ... Check some Alpha Values.

alpha = 10^seq(from =-3, to = 0, by = .1) # Goes from 0.01 to 1.

# A .1 improvement on 0.  A 1 No improvement from 0.1.
# Check between 0 and .1 ... and .1 and 1
startprocess <- Sys.time()

# Logging the results from Ridge Best Results as a lasso result as 0 Alpha not included in lasso results.
lassoResults <- rbind(lassoResults, data.table(err_id = nrow(lassoResults)+1,model = "lasso", alpha = 0, lambda = lambda_,train_error = err_value_));

for(alpha_value in alpha){
# Train the model using all the data
    startprocess <- Sys.time()
    final.model <- caret::train(price ~ ., data = training,
    method = "glmnet",
    metric = err_type_,
    trControl=train_control_config,
    tuneGrid = expand.grid(alpha = alpha_value, lambda = lambdas))
    
    alpha_ = final.model$bestTune[1,1]
    lambda_ = final.model$bestTune[1,2]
    
    final.pred <- predict(final.model, validation)
    final.pred[is.na(final.pred)]
    
    # # Get errors - difference from Train to Val.
    predictions_train <- final.pred
    train_ColToAnalyse  <-  validation[, "price"];

    errors_train <- predictions_train - train_ColToAnalyse;
    
    # Compute Metrics
    train_error_value <- get_error_value(predictions_train, train_ColToAnalyse); 
    
    # Logging the lasso results.
    lassoResults <- rbind(lassoResults, data.table(err_id = nrow(lassoResults)+1,model = "lasso", alpha = alpha_, lambda = lambda_,train_error = train_error_value));

}

  # Choosing the best alpha and lambda for the best training error in lasso.
    alpha_ = lassoResults[lassoResults$train_error == min(lassoResults$train_error)]$alpha[1]
    lambda_ = lassoResults[lassoResults$train_error == min(lassoResults$train_error)]$lambda[1]
    
    print (lassoResults[lassoResults$train_error == min(lassoResults$train_error)])
    
    lassoResults <- NULL
    
    # Now rerunning the Lasso for the best alpha / lambda combination.
    final.model <- caret::train(price ~ ., data = training,
      method = "glmnet",
      metric = err_type_,
      trControl=train_control_config,
      tuneGrid = expand.grid(alpha = alpha_, lambda = lambda_))  
    
    final.pred <- predict(final.model, validation)
    final.pred[is.na(final.pred)]
    

    pred_ <- predict(final.model, validation)
    pred_[is.na(pred_)] <- 0
    real_ <- validation$price
  
    # Compute Metrics
    rmse_ <- rmse(real_, pred_)
    mape_ <- mape(real_, pred_)
    
    # Error in terms of the mean deviation between the predicted value and the price of the houses
    mae_ <- mae(real_, pred_)
    
    # Now logging the best lasso model to the models table for later consolidation
    colResults <<- rbind(colResults, data.table(err_id = nrow(colResults)+1, model = "lasso", alpha = alpha_, rmse = rmse_, mae=mae_, mape=mape_));
  
    # storing the lasso model for later use.
    lasso_mod <- final.model

```
### Best Ridge / Lasso Results
Alpha  : 0.1258925	
Lambda : 0.001
Error  : 0.1317937

## Running some final models
Additionally to Baseline Linear models, Ridge and Lasso, we will also trying Logistic Regression, Decision Tree, Random Forest and XG Boost

### Starting with Logistic Regression

```{r checking_Error_Logistic, message=FALSE, warning=FALSE}
startprocess <- Sys.time()

glm_model_rtn<-run_model("glm", "glm")
glm_model <- glm_model_rtn$model
glm_model.pred <- glm_model_rtn$pred
hist(glm_model.pred, main="Histogram of Predictions : Logistic Regression", xlab = "Predictions")


```

## Decision Tree

```{r checking_Error_Decision, message=FALSE, warning=FALSE}
startprocess <- Sys.time()
d_tree_rtn <- run_model("rpart", "rpart")
d_tree <- d_tree_rtn$model
d_tree.pred <- d_tree_rtn$pred
hist(d_tree.pred, main="Histogram of Predictions : Decision Tree", xlab = "Predictions")
```

## Random Forest

```{r checking_Error_RF, message=FALSE, warning=FALSE}
startprocess <- Sys.time()
rf_tree_rtn <- run_model("rf", "rf")
rf_tree <- rf_tree_rtn$model
rf_tree.pred <- rf_tree_rtn$pred
hist(rf_tree.pred, main="Histogram of Predictions : Random Forest", xlab = "Predictions")
```

## XGBoost

```{r checking_Error_XG, message=FALSE, warning=FALSE}
startprocess <- Sys.time()
xgb_model_rtn <- run_model("xgboost", "xgboost")
xgb_model <- xgb_model_rtn$model
xgb_model.pred <- xgb_model_rtn$pred
hist(rf_tree.pred, main="Histogram of Predictions : XGBoost", xlab = "Predictions")

```

## Printing final model results ...


```{r print_final_error_table, message=FALSE, warning=FALSE}

print(colResults)

```


## Interpreting Error for all the models run and generating predictions ...
We now take the information and extract the details in order to make the final most accurate predictions

```{r generating_results_files, message=FALSE, warning=FALSE}
# First get the best model based on the error.
best.results <- get_best_model(err_type_);

# Extract the information needed to make final predictions
best.model <- best.results$model[1];
best.mape <- best.results$rmse[1]
best.mae <- best.results$mae[1]
best.rmse <- best.results$rmse[1]

predictions <- 0
val_found=F

if (tolower(best.model)=="glm"){
      # Fit a glm model to the input training data
  predictions = predict(glm_model, test)
  val_found=T
}
if (tolower(best.model)=="rf"){
        # Fit a Random Forest model to the input training data
  predictions = predict(rf_tree$finalModel, test)
  val_found=T 
}
if (tolower(best.model)=="rpart"){
        # Fit a Random Forest model to the input training data
  predictions = predict(d_tree$finalModel, test)
  val_found=T   
}
if (tolower(best.model)=="xgboost"){

  predictions = predict(xgb_model, test)
  val_found=T 
  
}
if((val_found==F) && (substr(tolower(best.model),1, 8)=="baseline")){
  
  # For Baseline, should always be last lm_model results.
  predictions = predict(lm_model, test)
  val_found=T 
  
}
if((val_found==F) && (substr(tolower(best.model),1, 5)=="ridge")){

  predictions = predict(ridge.mod, test)
  val_found=T 
  
}
if((val_found==F) && (substr(tolower(best.model),1, 5)=="lasso")){

  predictions = predict(lasso_mod, test)
  val_found=T 
  
}

```


### Now for file generation
Generating the predictions file : AlanCarton_HousePrice_Predictions.csv

```{r generating_file, message=FALSE, warning=FALSE}

if (val_found==T){
  predictions = as.numeric(exp_value(predictions, use_log)) 
  final_submission <- data.frame(id = original_test_data$id, price= (predictions))
  colnames(final_submission) <-c("id", "price")
  write.csv(final_submission, file = str_output_file, row.names = FALSE) 
}

```

### Details of how long the modelling took to run

```{r generating_times, message=FALSE, warning=FALSE}

print("Start Time")
print_time(starttime)
print("End Time")
print_time(Sys.time())

print_elapsed(starttime)

```

# Final Results

After data cleaning, columns for the final model were selected as a result of the minimum error increased after their removal.  If the error decreased after removal, the columns were dropped for the final model, otherwise the original columns were retained.

Further models were run with Linear Regression, Decision Tree, Random Forest and XGBoost, and overall the final module used for the predictions and the generation of the final files was chosen based on the lowest error for each model run.  Cross Validation was also used, and the best model was chosen based on the chosen error.

The error code used was set using the err_type_ variable at the top of the code.  Main values were RMSE and MAPE.  MAE was also possible but in all cases the MAE was 0.

Originally the code was run using RMSE, but on review the assignment called for MAPE, so the code was rerun.  Both sets of final details

The models took 35 minutes to run On i5 with 8gb Ram.

## Error Feature Selection and final model selection.

### Lowest results produced 
With feature engineering through MAPE
```{r print.best.results, message=FALSE, warning=FALSE}

best.results

```

### Columns used 

```{r final_columns, message=FALSE, warning=FALSE}

colnames(training[, colnames(training) != 'price'])

```  

## Best model 
Best results were obtained by selecting the columns using MAPE / and either MAPE or RMSE for model selection, and All Columns were used including additional features.  

In both instances XGBoost was the best model, the difference for the final dataset being as a result of the column selection as a result of MAPE or RMSE, with the best combination being MAPE selection, and either RMSE or MAPE error checking.

MAPE for final results 0.1280414 MAPE or 114815.2 RMSE.